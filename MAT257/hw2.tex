\documentclass{article}
\usepackage[margin=1.0in]{geometry}
\usepackage{amssymb,amsmath,amsthm,amsfonts}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{mathtools}

\newcommand{\Z}{\mathbf{Z}}
\newcommand{\N}{\mathbf{N}}
\newcommand{\R}{\mathbf{R}}
\newcommand{\Q}{\mathbf{Q}}
\newcommand{\C}{\mathbf{C}}

\newcommand{\id}{\mathrm{id}}
\newcommand{\op}{\mathrm{op}}
\newcommand{\diam}{\mathrm{diam}}
\newcommand{\Tr}{\mathrm{Tr}}
\newcommand{\im}{\mathrm{im}}
\newcommand{\rank}{\mathrm{rank}}

\newcommand{\cl}[1]{\overline{#1}}

\swapnumbers % places numbers before thm names

\theoremstyle{plain} % The "plain" style italicizes all body text.
	\newtheorem{thm}{Theorem}
		\numberwithin{thm}{section} % Theorem numbers are determined by section.
	\newtheorem{lemma}[thm]{Lemma}
	\newtheorem{prop}[thm]{Proposition}
	\newtheorem{cor}[thm]{Corollary}

\theoremstyle{definition}
    \newtheorem{defn}[thm]{Definition}
	\newtheorem{example}[thm]{Example}
	\newtheorem{exercise}[thm]{Exercise} %Exercise

\begin{document}
    \section{Week 2 Homework - Ethan Hua}
    \begin{exercise}
        \begin{enumerate}[label=(\alph*)]
            \item Prove that there exists an infinitely differentiable function $\alpha:\R\rightarrow \R$ such that $\alpha(t)=0$ for all $t\leq 0$, and $\alpha(t)>0$ for all $t>0$.
    
            \begin{proof}
                We define \(\alpha (t) =
                \begin{dcases}
                    0, &\text{ if } t \leq 0 ;\\
                    e^{-\frac{1}{t}} , &\text{ if } t > 0.\\
                \end{dcases}\) 
                
                Trivially \(\alpha (t) = 0 \text{ if } t \leq 0\) and \(\alpha (t) > 0 \text{ if }t > 0\). It remains to show that \(\alpha \) is infinitely differentiable.

                Since \(0\) is infinitely differentiable, and \(e^{-\frac{1}{x}}\) is infinitely differentiable for \(x > 0\), it suffices to show that derivatives of all orders of \(\alpha \) are continuous at \(t = 0\).

                We will prove using induction that
                \[
                    \forall n \in \mathbb{N} \cup \{0\} \text{, } \lim_{t \to 0} \alpha ^{(n)} (t) = 0
                \]

                First, we will show that 
                We will only worry about the right hand limit, as the left hand limit always evaluates to \(0\).

                For \(n = 0\),
                \[
                    \lim_{t \to 0^+} \alpha ^{(0)} (t) = \lim_{t \to 0^+} \alpha (t) = \lim_{t \to 0^+} e^{-\frac{1}{x}} = 0
                \]
                Thus the case for \(n = 0\) holds.

            \end{proof}
    
            \item Prove that there exists an infinitely differentiable function $\beta:\R\rightarrow \R$ such that $\beta(t)=1$ for all $t\geq 1$, and $\beta(t)=0$ for all $t\leq 0$.
    
            \textcolor{blue}{Hint: The shape you're looking for is $\dfrac{X}{X+Y}$.}
    
            \item Prove that there exists an infinitely differentiable function $\varphi:\R\rightarrow \R$ such that $\varphi(t)=1$ for all $t\in [2,3]$, and $\varphi(t)=0$ for $t\in \R\setminus (1,4)$.
    
            \textcolor{blue}{Hint: Your function $\beta(t)$ does half the job. Make a function $\gamma(t)$ that does the other half of the job. Then multiply them together.}
        \end{enumerate}
    \end{exercise}
    \begin{exercise}
        Let $S\subseteq \R^n$. Consider the following three statements:
        \begin{itemize}
            \item $S$ is a bounded subset of $(\R^n,\|\cdot\|_1)$.
            \item $S$ is a bounded subset of $(\R^n,\|\cdot\|_2)$.
            \item $S$ is a bounded subset of $(\R^n,\|\cdot\|_{\max})$.
        \end{itemize}
        Among these statements, determine which implications are true and which are false. There are six implications to investigate. Supply proof or counterexample as appropriate. Include pictures.
        \\\\
        \noindent Denote each statement as (1), (2), and (3), respectively. We claim that each statement implies all the other statements.
        \begin{proof}
            In this proof, we denote:
            \begin{enumerate}
                \item an open ball with respect to the 1-norm as \(B_1(p, \varepsilon )\)
                \item an open ball with respect to the Euclidean norm as \(B_2(p, \varepsilon )\) 
                \item an open ball with respect to the max-norm as \(B_{\max} (p, \varepsilon )\)  
            \end{enumerate}
            \((1) \implies (2)\):

            Suppose (1) is true. There exists a \(p = (p_1, p_2, \dots, p_n) \in \mathbb{R}^n\) and \(r > 0\) such that
            \[
                S \subseteq B_1(p, r)
            \] 
            We will show that \(S \subseteq B_2(p, r)\). Let \(x = (x_1, x_2, \dots, x_n) \in S\). Since \(S \subseteq B_1(p, r)\), by the triangle inequality,
            \[
                \left\lVert x - p \right\rVert _2 = \sqrt{\sum_{i=1}^{n} (x_i - p_i)^2} \leq \sum_{i=0}^n \left\vert x_i - p_i \right\vert  = \left\lVert x - p \right\rVert _1 < r
            \]
            Thus \(x \in B_2(p, r)\).

            \noindent\((2) \implies (1)\):
            
            We will show using induction on \(n\) that if for some open ball in \(\mathbb{R} ^n\) , \(S \subseteq B_2(p, r)\),  then \(S \subseteq B_1(p, \sqrt{n} r )\) 

            Let \(n = 1\). Suppose (2) is true. Fix \(x \in S \subseteq \mathbb{R} \). Then by (2),
            \[
                r > \left\lVert x - p \right\rVert _2 = \sqrt{(x - p)^2} = \left\vert x - p \right\vert = \left\lVert x - p \right\rVert _1 \implies S \subseteq B_1(p, \sqrt{1} \cdot r) 
            \]
            Thus the claim holds for \(n = 1\). Now suppose the claim holds for \(n = k\). Assume (2) is true. Let \(x \in S\). By (2),
            \[
                \left\lVert x - p \right\rVert _2 = \sqrt{\sum_{i=1}^{k+1} (x_i - p_i)^2} < r \implies \sum_{i=1}^{k+1} (x_i - p_i)^2 < r^2\tag{*}
            \]
            We want to show that \(x \in B_1(p, \sqrt{n} r)\), or
            \[
                \left\lVert x - p \right\rVert _1 = \sum_{i=0}^{k+1} \left\vert x_i - p_i \right\vert < \sqrt{n} r \iff \left(\sum_{i=0}^{k+1} \left\vert x_i - p_i \right\vert\right)^2 < nr^2 
            \]
            We have
            \[
                \left(\sum_{i=0}^{k+1} \left\vert x_i - p_i \right\vert\right)^2 = \left(\sum_{i=0}^{k} \left\vert x_i - p_i \right\vert + \left\vert x_{k+1} - p_{k+1} \right\vert \right)^2
            \]
            \[
                = \left(\sum_{i=0}^{k} \left\vert x_i - p_i \right\vert\right)^2 + 2\left\vert x_{k+1} - p_{k+1} \right\vert \left(\sum_{i=0}^{k} \left\vert x_i - p_i \right\vert\right) + \left\vert x_{k+1} - p_{k+1} \right\vert ^2 \tag{**}
            \]
            Manipulating the inequality from (*), we obtain
            \[
                \sum_{i=1}^{k} (x_i - p_i)^2 < r^2 - \left\vert x_{k+1} - p_{k+1}  \right\vert ^2
            \]
            which implies that \((x_1, x_2, \dots , x_k)\) is in the open ball \(B_2(p, r)\) on \(\mathbb{R} ^k\). From the indudction hypothesis,
            \[
                (**) < k(r^2 - \left\vert x_{k+1} - p_{k+1}  \right\vert ^2) + 2\left\vert x_{k+1} - p_{k+1} \right\vert \left(\sum_{i=0}^{k} \left\vert x_i - p_i \right\vert\right) + r^2 - \sum_{i=1}^{k} (x_i - p_i)^2
            \]
            \[
                = (k+1)r^2 - \left(k\left\vert x_{k+1} - p_{k+1}  \right\vert ^2 - 2\left\vert x_{k+1} - p_{k+1} \right\vert \left(\sum_{i=0}^{k} \left\vert x_i - p_i \right\vert\right) + \sum_{i=1}^{k} (x_i - p_i)^2\right)
            \]
            \[
                = (k+1)r^2 - \sum_{i=1}^k \left(\left\vert x_{k+1} - p_{k+1}  \right\vert ^2 - 2\left\vert x_{k+1} - p_{k+1} \right\vert\left\vert x_i - p_i \right\vert + (x_i - p_i)^2\right)
            \]
            \[
                = (k+1)r^2 - \sum_{i=1}^k \left(\left\vert x_{k+1} - p_{k+1}  \right\vert - \left\vert x_i - p_i \right\vert \right) ^2 \leq (k+1)r^2
            \]
            In summary, we have that
            \[
                \left\lVert x - p \right\rVert _1 < \sqrt{k+1} r
            \]
            Thus \(x\) is in the open ball \(B_1(p, \sqrt{k+1} r)\), which means that (1) is true. By the principle of induction, the claim holds for all \(n \in \mathbb{N} \). 

            \noindent\((1) \implies (3)\):
            Suppose (1). Then \(S\) is a subset of some open ball \(B_1(p, r)\). We will show that \(S \subseteq B_{max} (p, r)\).
            
            Let \(x \in S\). Then by (1):
            \[
                \left\lVert x - p \right\rVert _{\max} = \max_{1 \leq i \leq n} \{\left\vert x_i - p_i \right\vert \} \leq \sum_{i=1} ^n \left\vert x_i - p_i \right\vert = \left\lVert x - p \right\rVert _1 < r
            \]
            Thus \(x \in B_{\max} (p, r)\), so (3) is true.
            
            \noindent \((3) \implies (1)\):

            Suppose that (3) holds true. Then for some open ball \(B_{\max}(p, r)\). We will show that \(S \subseteq B_1(p, nr)\).
            
            Let \(x \in S\). We have
            \[
                \left\lVert x - p \right\rVert _{1} = \sum_{i=1} ^n \left\vert x_i - p_i \right\vert \leq n\max_{0 \leq i \leq n} {\left\vert x_i - p_i \right\vert } = n \left\lVert x - p \right\rVert _{\max} < nr
            \]
            Thus \(x \in B_1(p, nr)\), which implies that (1) is true.

            \noindent \((2) \implies (3)\):
            
            Suppose (2) is true. Then \(S \subseteq B_2(p,r)\), where \(B_2(p, r)\) is some open ball on the Euclidean norm. We want to show that \(S \subseteq B_{\max }(p, r)\). Indeed, we have
            \[
                \left\lVert x - p \right\rVert _{\max } = \max _{1 \leq i \leq n} \left\vert x_i - p_i \right\vert \leq \sqrt{\sum_{i=1} ^n \left\vert x_i - p_i \right\vert ^2} < r 
            \] 
            Thus \(S \subseteq B_{\max}(p, r)\), so (3) is true.
            
            \noindent \((3) \implies (2)\):
            
            Suppose that (3) holds true. Then for some open ball with the max-norm, \(S \subseteq B_{\max}(p, r)\). We will show that \(S \subseteq B_2(p, nr)\)
            
            Let \(x \in S\). Then be the triangle inequality,
            \[
                \left\lVert x - p \right\rVert _2 = \sqrt{\sum_{i = 1}^n \left\vert a_i - p_i \right\vert } \leq \sum_{i=1}^n \left\vert a_i - p_i \right\vert \leq n\max_{1 \leq i \leq n} \left\vert x_i - p_i \right\vert < n \left\lVert x - p \right\rVert _{\max} < nr
            \]
            Thus \(x \in B_2(p,nr)\), so (2) holds.
            
            \noindent Therefore, we have proven every implication to be true.

        \end{proof}
    \end{exercise}
    \begin{exercise}
        Let $(X,\|\cdot\|_X)$ and $(Y,\|\cdot\|_Y)$ be two normed vector spaces. A linear mapping $T:X\rightarrow Y$ is called \textbf{bounded} if there exists a constant $M\geq 0$ such that
        \[ \|T(x)\|_Y \leq M \|x\|_X \quad \text{for all $x\in X$.}  \]
        Let $B(X,Y)$ denote the set of these bounded linear operators. The \textbf{operator norm} on $B(X,Y)$, denoted by $\|\cdot\|_{\mathrm{op}}$, is defined as follows:
            \[ \|T\|_{\mathrm{op}} = \sup\{ \|T(x)\|_Y : x\in X \text{ and } \|x\|_X\leq 1 \}. \]
        \begin{enumerate}[label=(\alph*)]
            \item Prove that $B(X,Y)$ is a linear subspace of $L(X,Y)$.
            
            \begin{proof}
                The 0-transformation \(\left\lVert Z(x) \right\rVert  = 0 \leq \left\lVert x \right\rVert _X \text{, } \forall x \in X\), because of the definition of a norm. Thus \(0 \in B(X,Y)\).
                Let \(T, U \in B(X,Y) \text{, } c \in \mathbb{R}\). Then for all \(x \in X\), there exist \(M, N \geq 0\) such that
                \[
                    T(x) \leq M \left\lVert x \right\rVert _X \text{ and } U(x) \leq N \left\lVert x \right\rVert _X \implies T(x) + U(x) \leq (M+N) \left\lVert x \right\rVert _X
                \] 
                Which implies that \(T+U\) is a member of \(B(X,Y)\). As well, from the first inequality,
                \[
                    T(x) \leq M \left\lVert x \right\rVert _X \implies cT(x) \leq  cM \left\lVert x \right\rVert _X
                \]
                Which implies that \(cT\) is a member of \(B(X,Y)\).

                Since \(0 \in B(X,Y)\)and \(B(X,Y)\) is closed under addition and scalar multiplication, \(B(X,Y)\) is a subspace of \(L(X,Y)\).

            \end{proof}
            \item Prove that $\|\cdot\|_{\mathrm{op}}$ is a norm on $B(X,Y)$.
        
            \begin{proof}
                To prove that the operator norm is a norm, we first verify that \(\left\lVert T \right\rVert _{op} = 0 \iff T = 0\).

                Fix \(T \in B(X,Y)\) and suppose that \(T = 0\). Then for all \(x \in X \text{, } \left\lVert x \right\rVert _X \leq 1 \text{, } \left\lVert T(x) \right\rVert _Y = \left\lVert 0 \right\rVert _Y = 0\). Thus \(\left\lVert T \right\rVert _{op} = 0\).  
                
                Now suppose the converse, that \(\left\lVert T \right\rVert _{op} = 0\). Then 
                \[
                    \forall x \in X, \left\lVert x \right\rVert _X \leq 1, \left\lVert T(x) \right\rVert _Y \leq 0.
                \]
                But by the definition of the norm in Y,
                \[
                    0 \leq \left\lVert T(x) \right\rVert _Y
                \]
                It follows that \(T(x) = 0\).

                We denote the set of elements \(x\) in \(X\) such that \(\left\lVert x \right\rVert _X \leq 1\) as \(X^\prime\).

                To show nonnegativity, we note that for \(x \in X^\prime\),
                \[
                    \left\lVert T \right\rVert _{op} \geq \left\lVert T(x) \right\rVert _Y \geq 0
                \]

                To show homogeity, let \(T \in G(X,Y), c \in \mathbb{R}\). Then
                \[
                    \left\lVert cT \right\rVert _{op} = \sup \{ \left\lVert cT(x) \right\rVert _Y : x \in X^\prime\} = \sup \{ c\left\lVert T(x) \right\rVert _Y : x \in X^\prime\} = c\sup \{ \left\lVert T(x) \right\rVert _Y : x \in X^\prime\} = c \left\lVert T \right\rVert _{op}
                \] 

                Now we show that the triangle inequality holds with respect to the operator norm.

                Fix \(T, U \in B(X,Y)\). We denote the set of elements \(x\) in \(X\) such that \(\left\lVert x \right\rVert _X \leq 1\) as \(X^\prime\).
                Let \(x \in X^\prime\). By definition,
                \[
                    T(x) \leq \sup T(X^\prime) \text{ and } U(x) \leq \sup U(X^\prime)
                \]
                Adding both together obtains
                \[
                    T(x) + U(x) \leq \sup T(X^\prime) + \sup U(X^\prime)
                \]
                We see that \(\sup T(X^\prime) + \sup U(X^\prime)\) is an upper bound for \(T(x) + U(x)\). By the definition of the least upper bound,
                \[
                    \sup \{T(X^\prime)+U(X^\prime)\} \leq  \sup T(X^\prime) + \sup U(X^\prime) \implies \left\lVert T+U \right\rVert _{op} \leq \left\lVert T \right\rVert _{op} + \left\lVert U \right\rVert _{op}
                \]
                Thus the operator norm is, indeed, a norm.

            \end{proof}
            \item Let $T:\R^2\rightarrow \R^2$ be the linear mapping given by $T(x,y)=(x+y,x)$. Find, with proof, the exact value of $\|T\|_{\mathrm{op}}$. (Here, $\R^2$ is equipped with the usual norm.)
        
            \item Find, with proof, an example of an unbounded linear operator.
            
            \begin{proof}
                Let \(X=Y=\mathbb{R} ^n\), \(\left\lVert \cdot \right\rVert _X=\left\lVert \cdot \right\rVert _2\) and
                \(\left\lVert \cdot \right\rVert _Y=\left\lVert \cdot \right\rVert _{\max} \)
                
                Let \(T(x) = x\). Fix \(M \geq 0\). 
            \end{proof}
        \end{enumerate}
    \end{exercise}
\end{document}